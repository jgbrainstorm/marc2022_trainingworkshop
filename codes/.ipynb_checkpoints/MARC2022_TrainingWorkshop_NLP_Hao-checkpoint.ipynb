{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARC 2022 Training Workshop on Machine Learning and NLP \n",
    "## Part II: NLP\n",
    "\n",
    "### Jiangang Hao, ETS, contact: <jhao@ets.org>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text preprocessing and Ngram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The class is over. I hopep it is intersting to you. Please let me knoww if not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to lower case\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and punctuations\n",
    "stopword_list = stopwords.words('english')\n",
    "punctuation_list = list(string.punctuation)\n",
    "cleaned_text = [txt for txt in word_tokenize(text.lower()) if txt not in stopword_list+punctuation_list]\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typo correction\n",
    "spell = SpellChecker()\n",
    "corrected_text = [spell.correction(wd) for wd in cleaned_text]\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech tagging\n",
    "pos_tag(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the words\n",
    "porter = PorterStemmer()\n",
    "stem_words = [porter.stem(txt) for txt in corrected_text]\n",
    "list(zip(corrected_text,stem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "sentence_list = sent_tokenize(text)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the stop words removal and typo correction\n",
    "correct_sentence_list = []\n",
    "for sent in sentence_list:\n",
    "    correct_sentence_list.append(' '.join([spell.correction(wd) for wd in word_tokenize(sent.lower()) \\\n",
    "                                  if wd not in stopword_list+punctuation_list]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1)) \n",
    "X = vectorizer.fit_transform(correct_sentence_list)\n",
    "df = pd.DataFrame(X.toarray())\n",
    "df.columns = vectorizer.get_feature_names_out()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-Idf transformation of unigram\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1)) \n",
    "X = vectorizer.fit_transform(correct_sentence_list)\n",
    "df = pd.DataFrame(X.toarray())\n",
    "df.columns = vectorizer.get_feature_names_out()\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(sentence_list)\n",
    "df = pd.DataFrame(X.toarray())\n",
    "df.columns = vectorizer.get_feature_names_out()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Latent Semantic Analysis\n",
    "Here is a great tutorial for more details for using Gensim: <https://www.datacamp.com/tutorial/discovering-hidden-topics-python>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents from communication in a collaborative task\n",
    "doc_list=[\"So, apt. A has mail + packages delivered directly to the tenants, guaranteed same rent for 2 years, and free wifi\", \n",
    "          \"I like a as well\",\"I did pick C as the best however A also has several friends that live in the same building\", \n",
    "          \"all utilities are included, and it includes 2 full bathrooms\", \"A or C is my pick, because B's rent apparently usually increases 20% after the 1st yr\",\n",
    "          \"It also has onsite laundry which is clean and well maintained and available 24 7\", \"I think I'd go with A, then\", \"I think that the availability of  maintenance makes C the best\",\n",
    "          \"Can we all agree that B is the worst?\", \"Apartment B has tenants next door with a salt water aquarium, cell phone service connectivity is weak, and there is only one full bathroom in the apartment\",\n",
    "          \"C says maintenance isn't always handled promptly, so I'm worried about it\", \"B is def worst\", \"so yeah I dislike B\", \"A,C,B then?\", \"that weak cell phone service is enough to make me not even consider it lol\",\n",
    "          \"Yeah utilities arent included for b\", \"A, C, B is definitely what I'd do, yeah\", \"I'd agree\", \"Apartment C offers a discount for local college students, the complex is located on land that used to be a farm, and the landlord owns a pizza restaurant\",\n",
    "          \"i agree with A, C, B\", \"The 20% rent increase after a year for B is a big oof\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying preprocessing\n",
    "\n",
    "doc_list_correct = []\n",
    "for sent in doc_list:\n",
    "    doc_list_correct.append(' '.join([spell.correction(wd) for wd in word_tokenize(sent.lower()) \\\n",
    "                                  if wd not in stopword_list+punctuation_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the doc term matrix\n",
    "vectorizer_lsa = TfidfVectorizer(ngram_range=(1,1)) \n",
    "X_lsa = vectorizer_lsa.fit_transform(doc_list_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document term matrix\n",
    "pd.DataFrame(X_lsa.toarray()).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of topics and create SVD object\n",
    "num_components=10\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVD model on data\n",
    "lsa.fit_transform(X_lsa)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_ \n",
    "V_T = lsa.components_.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained variance by topics\n",
    "plt.plot(lsa.explained_variance_ratio_,'bo-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics with their terms\n",
    "terms = vectorizer_lsa.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "\n",
    "def print_topics(lsa_model):\n",
    "    for index, component in enumerate(lsa_model.components_):\n",
    "        zipped = zip(terms, component)\n",
    "        top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "        top_terms_list=list(dict(top_terms_key).keys())\n",
    "        print(\"Topic \"+str(index)+\": \",top_terms_list)\n",
    "print_topics(lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neural Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors (word2vec)\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the 100 dimension word vector dictionary trained on twitter data. https://nlp.stanford.edu/projects/glove/\n",
    "model = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the word cat\n",
    "model.get_vector('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words as cat\n",
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between cat and tiger\n",
    "1-cosine(model.get_vector('cat'), model.get_vector('tiger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity between cat and kitten\n",
    "1-cosine(model.get_vector('cat'), model.get_vector('kitten'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarit between cat and car\n",
    "1-cosine(model.get_vector('cat'), model.get_vector('car'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deep Learning Language Models\n",
    "<https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the blank task\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(unmasker(\"Hello I'm Jiangang, and I am running a <mask> to teach people machine learning.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another one, A: ordinary, B: stubborn, C: skeptical, D. fascinating, E. unobtrusive\n",
    "pd.DataFrame(unmasker(\"It is ironic and somehow tragic that good people are often dull while evil people can be endlessly <mask>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence generation\n",
    "from transformers import pipeline, set_seed\n",
    "#generator = pipeline('text-generation', model='gpt2')\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
    "set_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='Today, I am giving a traing workshop on machine learning and NLP. I am going to'\n",
    "print(generator(prompt, max_length=120, num_return_sequences=1)[0].get('generated_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
